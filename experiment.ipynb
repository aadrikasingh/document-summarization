{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a file url\n",
    "file = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "Starting Processing https://storageas002.blob.core.windows.net/kmoaidemo/Benefit_Options.pdf?sp=r&st=2023-06-19T07:02:46Z&se=2023-07-01T15:02:46Z&spr=https&sv=2022-11-02&sr=b&sig=6myaamyMosPNZqx10jLTcsLT%2F%2B34PlqdeibUyE8aeCk%3D ...\n",
      "Reading https://storageas002.blob.core.windows.net/kmoaidemo/Benefit_Options.pdf?sp=r&st=2023-06-19T07:02:46Z&se=2023-07-01T15:02:46Z&spr=https&sv=2022-11-02&sr=b&sig=6myaamyMosPNZqx10jLTcsLT%2F%2B34PlqdeibUyE8aeCk%3D\n",
      "Chunks Generated 1  | max_tokens 6574  | Chunk Lengths: 898\n",
      "Done Processing https://storageas002.blob.core.windows.net/kmoaidemo/Benefit_Options.pdf?sp=r&st=2023-06-19T07:02:46Z&se=2023-07-01T15:02:46Z&spr=https&sv=2022-11-02&sr=b&sig=6myaamyMosPNZqx10jLTcsLT%2F%2B34PlqdeibUyE8aeCk%3D in 21.403057098388672 seconds\n",
      "##########################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_summary = summarization.summarize_document(file, mode='refine', verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Contoso Electronics offers two health insurance plans through Northwind Health: Northwind Health Plus and Northwind Standard. Northwind Health Plus is a comprehensive plan that covers medical, vision, and dental services, prescription drugs, mental health and substance abuse, and preventive care services. It also offers coverage for emergency services, both in-network and out-of-network. On the other hand, Northwind Standard is a basic plan that covers medical, vision, and dental services, prescription drugs, and preventive care services. However, it does not offer coverage for emergency services, mental health and substance abuse, or out-of-network services.\\n\\nBoth plans cover routine physicals, well-child visits, immunizations, and other preventive care services, as well as preventive care services such as mammograms, colonoscopies, and other cancer screenings. However, Northwind Health Plus offers more comprehensive coverage than Northwind Standard. It covers a wider range of prescription drugs, including generic, brand-name, and specialty drugs, while Northwind Standard only covers generic and brand-name drugs. It also covers vision exams, glasses, and contact lenses, as well as dental exams, cleanings, and fillings, while Northwind Standard only covers vision exams and glasses.\\n\\nThe cost of the health insurance will be deducted from each paycheck, and the employee's portion of the cost will be calculated based on the selected health plan and the number of people covered by the insurance. The cost per paycheck for Northwind Standard ranges from $45.00 for employee-only coverage to $78.00 for employee +2 or more coverage. On the other hand, the cost per paycheck for Northwind Health Plus ranges from $55.00 for employee-only coverage to $89.00 for employee +2 or more coverage. \\n\\nOverall, Contoso Electronics offers two health insurance plans that cater to different needs and budgets. Employees can choose between a comprehensive plan that offers more coverage or a basic plan that covers essential services. The cost of the health insurance is spread out over the course of the year, making it more manageable for employees.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_summary['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################\n",
      "Starting Processing https://storageas002.blob.core.windows.net/kmoaidemo/Benefit_Options.pdf?sp=r&st=2023-06-19T07:02:46Z&se=2023-07-01T15:02:46Z&spr=https&sv=2022-11-02&sr=b&sig=6myaamyMosPNZqx10jLTcsLT%2F%2B34PlqdeibUyE8aeCk%3D ...\n",
      "Reading https://storageas002.blob.core.windows.net/kmoaidemo/Benefit_Options.pdf?sp=r&st=2023-06-19T07:02:46Z&se=2023-07-01T15:02:46Z&spr=https&sv=2022-11-02&sr=b&sig=6myaamyMosPNZqx10jLTcsLT%2F%2B34PlqdeibUyE8aeCk%3D\n",
      "Chunks Generated 1  | max_tokens 7152  | Chunk Lengths: 898\n",
      "I am here\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "memory=None callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x000001A0353A5390> verbose=False input_key='input_documents' output_key='output_text' llm_chain=LLMChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x000001A0353A5390>, verbose=False, prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='The maximum output is about 500 to 750 tokens, so make sure to take advantage of this to the maximum.\\n\\nWrite an elaborate summary of 3 paragraphs of the following:\\n\\n\\n{text}\\n\\n\\nSUMMARY:', template_format='f-string', validate_template=True), llm=ChatOpenAI(verbose=True, callback_manager=<langchain.callbacks.base.CallbackManager object at 0x000001A02275F4C0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-35-turbo', temperature=0.0, model_kwargs={'model': 'gpt-35-turbo', 'engine': 'gpt-35-turbo'}, openai_api_key='de33a658def9465ea34c1d790d29b4a8', openai_organization=None, request_timeout=120, max_retries=30, streaming=False, n=1, max_tokens=500), output_key='text') combine_document_chain=StuffDocumentsChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x000001A0353A5390>, verbose=False, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x000001A0353A5390>, verbose=False, prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='The maximum output is about 500 to 750 tokens, so make sure to take advantage of this to the maximum.\\n\\nWrite an elaborate summary of 3 paragraphs of the following:\\n\\n\\n{text}\\n\\n\\nSUMMARY:', template_format='f-string', validate_template=True), llm=ChatOpenAI(verbose=True, callback_manager=<langchain.callbacks.base.CallbackManager object at 0x000001A02275F4C0>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-35-turbo', temperature=0.0, model_kwargs={'model': 'gpt-35-turbo', 'engine': 'gpt-35-turbo'}, openai_api_key='de33a658def9465ea34c1d790d29b4a8', openai_organization=None, request_timeout=120, max_retries=30, streaming=False, n=1, max_tokens=500), output_key='text'), document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), document_variable_name='text') collapse_document_chain=None document_variable_name='text' return_intermediate_steps=True\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Could not automatically map gpt-35-turbo to a tokeniser. Please use `tiktok.get_encoding` to explicitly get the tokeniser you expect.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mp_summary \u001b[39m=\u001b[39m summarization\u001b[39m.\u001b[39;49msummarize_document(file, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmap_reduce\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\utils\\summarization.py:159\u001b[0m, in \u001b[0;36msummarize_document\u001b[1;34m(path, mode, verbose)\u001b[0m\n\u001b[0;32m    155\u001b[0m text \u001b[39m=\u001b[39m read_document(path, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[0;32m    157\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m summ \u001b[39m=\u001b[39m summarize_text(text, mode\u001b[39m=\u001b[39;49mmode, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[0;32m    160\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    162\u001b[0m summary \u001b[39m=\u001b[39m {\n\u001b[0;32m    163\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m'\u001b[39m: os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(path),\n\u001b[0;32m    164\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mintermediate_steps\u001b[39m\u001b[39m'\u001b[39m: summ[\u001b[39m'\u001b[39m\u001b[39mintermediate_steps\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    165\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m: summ[\u001b[39m'\u001b[39m\u001b[39moutput_text\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    166\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mproc_time\u001b[39m\u001b[39m'\u001b[39m: end\u001b[39m-\u001b[39mstart\n\u001b[0;32m    167\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\utils\\summarization.py:180\u001b[0m, in \u001b[0;36msummarize_text\u001b[1;34m(text, mode, verbose)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmap_reduce\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    179\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mI am here\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 180\u001b[0m     summ \u001b[39m=\u001b[39m get_mapreduced_summarization(docs)\n\u001b[0;32m    181\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid mode\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\utils\\summarization.py:120\u001b[0m, in \u001b[0;36mget_mapreduced_summarization\u001b[1;34m(docs, model, max_output_tokens, stream, callbacks)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[39mprint\u001b[39m(chain)\n\u001b[1;32m--> 120\u001b[0m summ \u001b[39m=\u001b[39m chain({\u001b[39m\"\u001b[39;49m\u001b[39minput_documents\u001b[39;49m\u001b[39m\"\u001b[39;49m: docs}, return_only_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    121\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[39mreturn\u001b[39;00m summ\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\.venv\\lib\\site-packages\\langchain\\chains\\base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\.venv\\lib\\site-packages\\langchain\\chains\\base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    109\u001b[0m     inputs,\n\u001b[0;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:56\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m     55\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[1;32m---> 56\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_docs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mother_keys)\n\u001b[0;32m     57\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[0;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:143\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \n\u001b[0;32m    136\u001b[0m \u001b[39mCombine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39mThis reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mapply(\n\u001b[0;32m    140\u001b[0m     \u001b[39m# FYI - this is parallelized and so it is fast.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     [{\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_variable_name: d\u001b[39m.\u001b[39mpage_content}, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs} \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m docs]\n\u001b[0;32m    142\u001b[0m )\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_results(results, docs, token_max, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:173\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain._process_results\u001b[1;34m(self, results, docs, token_max, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m result_docs \u001b[39m=\u001b[39m [\n\u001b[0;32m    168\u001b[0m     Document(page_content\u001b[39m=\u001b[39mr[question_result_key], metadata\u001b[39m=\u001b[39mdocs[i]\u001b[39m.\u001b[39mmetadata)\n\u001b[0;32m    169\u001b[0m     \u001b[39m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[39mfor\u001b[39;00m i, r \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(results)\n\u001b[0;32m    171\u001b[0m ]\n\u001b[0;32m    172\u001b[0m length_func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_document_chain\u001b[39m.\u001b[39mprompt_length\n\u001b[1;32m--> 173\u001b[0m num_tokens \u001b[39m=\u001b[39m length_func(result_docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_collapse_docs_func\u001b[39m(docs: List[Document], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    176\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collapse_chain\u001b[39m.\u001b[39mrun(input_documents\u001b[39m=\u001b[39mdocs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:83\u001b[0m, in \u001b[0;36mStuffDocumentsChain.prompt_length\u001b[1;34m(self, docs, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     82\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mget_num_tokens(prompt)\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\.venv\\lib\\site-packages\\langchain\\chat_models\\openai.py:342\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to calculate get_num_tokens. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m     )\n\u001b[0;32m    341\u001b[0m \u001b[39m# create a GPT-3.5-Turbo encoder instance\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m enc \u001b[39m=\u001b[39m tiktoken\u001b[39m.\u001b[39;49mencoding_for_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name)\n\u001b[0;32m    344\u001b[0m \u001b[39m# encode the text using the GPT-3.5-Turbo encoder\u001b[39;00m\n\u001b[0;32m    345\u001b[0m tokenized_text \u001b[39m=\u001b[39m enc\u001b[39m.\u001b[39mencode(text)\n",
      "File \u001b[1;32mc:\\Users\\singhaadrika\\GitHub\\document-summarization\\.venv\\lib\\site-packages\\tiktoken\\model.py:70\u001b[0m, in \u001b[0;36mencoding_for_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[39mreturn\u001b[39;00m get_encoding(model_encoding_name)\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m encoding_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m     71\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not automatically map \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m to a tokeniser. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease use `tiktok.get_encoding` to explicitly get the tokeniser you expect.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mreturn\u001b[39;00m get_encoding(encoding_name)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Could not automatically map gpt-35-turbo to a tokeniser. Please use `tiktok.get_encoding` to explicitly get the tokeniser you expect.'"
     ]
    }
   ],
   "source": [
    "mp_summary = summarization.summarize_document(file, mode='map_reduce', verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_summary['summary']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
